<div class="content-section research-pipeline">
  <h1>Automatic Speech Recognition (ASR)</h1>
  <p>
    Automatic Speech Recognition (ASR) converts spoken language into written text by analyzing audio signals and recognizing speech patterns. 
    In SPROUT, ASR is the final step in our pipeline—turning child‐only segments into time‐aligned transcripts for linguistic and clinical analysis.
  </p>

  <!-- Navigation -->
  <nav class="pipeline-nav">
    <a href="#overview">1. Overview</a>
    <a href="#integration">2. ASR Pipeline Integration</a>
    <a href="#models">3. ASR Model Used</a>
    <a href="#outputs">4. Output Formats</a>
    <a href="#challenges">5. Challenges</a>
    <a href="#evaluation">6. Evaluation Summary</a>
    <a href="#notes">Notes</a>
  </nav>

  <!-- 1. Overview -->
  <section id="overview" class="subsection">
    <h2>1. Overview</h2>
    <p>
      ASR systems typically involve:
    </p>
    <ul>
      <li><strong>Audio preprocessing</strong>: extract ML-ready features (MFCCs, spectrograms)</li>
      <li><strong>Acoustic modeling</strong>: learn mapping from features → phonemes</li>
      <li><strong>Language modeling</strong>: predict likely word sequences</li>
      <li><strong>Decoding</strong>: combine models to produce final text</li>
    </ul>
  </section>

  <!-- 2. ASR Pipeline Integration -->
  <section id="integration" class="subsection">
    <h2>2. ASR Pipeline Integration</h2>
    <p>Our ASR module sits downstream of:</p>
    <pre><code>Raw Audio → Preprocessing → VAD & Silence Removal → Diarization → Segmentation → ASR → Analysis</code></pre>
    <p>
      - <strong>Diarization</strong> isolates child voices  
      - <strong>Segmentation</strong> breaks long audio into ≤30 s chunks  
      Together, these maximize transcription accuracy on pediatric speech.
    </p>
  </section>

  <!-- 3. ASR Model Used -->
  <section id="models" class="subsection">
    <h2>3. ASR Model Used</h2>
    <p>
      We leverage two state-of-the-art architectures:
      <strong>OpenAI Whisper</strong> (encoder–decoder transformer) and 
      <strong>wav2vec-2.0</strong> (self-supervised conv + transformer CTC).
    </p>

    <h3>Whisper Model Variants</h3>
    <input type="text" id="whisperFilter" class="table-filter" placeholder="Filter models…" />
    <table id="whisperTable">
      <thead>
        <tr><th>Model</th><th>Params</th><th>Speed</th><th>Use Case</th></tr>
      </thead>
      <tbody>
        <tr><td>tiny</td><td>39 M</td><td>Fastest</td><td>Real-time apps</td></tr>
        <tr><td>base</td><td>74 M</td><td>Fast</td><td>Quick processing</td></tr>
        <tr><td>small</td><td>244 M</td><td>Medium</td><td>Balanced</td></tr>
        <tr><td>medium</td><td>769 M</td><td>Slow</td><td>High accuracy</td></tr>
        <tr><td>large</td><td>1550 M</td><td>Slowest</td><td><strong>Project default</strong></td></tr>
        <tr><td>turbo</td><td>809 M</td><td>Optimized</td><td>Speed/accuracy</td></tr>
      </tbody>
    </table>

    <details class="code-details">
      <summary>View model initialization & inference</summary>
      <button class="copy-btn">Copy</button>
      <pre><code>### Whisper (OpenAI)
import whisper
model = whisper.load_model("large")
res = model.transcribe("segment.wav", language="en")
text = res["text"]

### wav2vec-2.0 (HuggingFace)
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch, soundfile as sf

audio, sr = sf.read("segment.wav")
proc  = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
mdl   = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")
inp   = proc(audio, sampling_rate=sr, return_tensors="pt", padding=True)
with torch.no_grad():
    logits = mdl(inp.input_values).logits
pred  = torch.argmax(logits, dim=-1)
hyp   = proc.batch_decode(pred)[0]</code></pre>
    </details>
  </section>

  <!-- 4. Output Formats -->
  <section id="outputs" class="subsection">
    <h2>4. Output Formats</h2>
    <ul>
      <li><strong>Transcripts</strong>: cleaned text per segment</li>
      <li><strong>Timestamps</strong>: start/end times for words (Whisper) or frames (wav2vec)</li>
    </ul>
    <button class="toggle-btn" data-target="#outputSample">Show/Hide Sample Output</button>
    <pre id="outputSample" class="csv-sample"><code>starting 0.0 ending 3.92
Ready? Say cash.
starting 4.32 ending 5.0
Cash.
…</code></pre>
  </section>

  <!-- 5. Challenges -->
  <section id="challenges" class="subsection">
    <h2>5. Challenges</h2>
    <ul>
      <li>Disfluent speech (pauses, repeats, stutters)</li>
      <li>Small vocabulary & mispronunciations (“pig”→“big”)</li>
      <li>Short utterances often get clipped or skipped</li>
    </ul>
  </section>

  <!-- 6. Evaluation Summary -->
  <section id="evaluation" class="subsection">
    <h2>6. Evaluation Summary</h2>
    <p>No ground-truth transcripts currently available, so we use internal QC metrics:</p>
    <ul>
      <li><strong>Avg. Confidence Score</strong>: model’s confidence per segment</li>
      <li><strong>Keyword Matching Accuracy</strong>: exact & fuzzy match on clinical prompts</li>
    </ul>
  </section>

  <!-- Notes -->
  <section id="notes" class="subsection">
    <h2>Notes</h2>
    <h3>1. Out-of-the-Box Approach (Current)</h3>
    <p>Use pre-trained Whisper without fine-tuning, optimizing upstream audio processing for best baseline performance on child speech.</p>
    <h3>2. Fine-tuning Whisper (Under Dev)</h3>
    <p>Apply PEFT/LoRA to adapt Whisper decoder layers to pediatric speech patterns, preserving encoder robustness.</p>
    <h3>3. Adversarial Debiasing (Under Dev)</h3>
    <p>Incorporate adversarial training to reduce demographic biases and equalize accuracy across dialects and developmental stages.</p>
  </section>
</div>

<style>
  .research-pipeline { font-family: 'Lato', sans-serif; line-height: 1.6; }
  .research-pipeline h1 { font-family: 'Montserrat', sans-serif; color: #4E2A84; }
  .pipeline-nav { text-align: center; margin: 1rem 0; }
  .pipeline-nav a { margin: 0 1rem; color: #4E2A84; text-decoration: none; font-weight: 600; }
  .content-section { background: #fff; padding: 2rem; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
  .subsection { margin-top: 2rem; }
  .subsection h2 { font-family: 'Montserrat', sans-serif; color: #4E2A84; border-bottom: 2px solid #FFD100; padding-bottom: .25rem; }
  .subsection h3 { color: #FFD100; margin-top: 1rem; }
  .table-filter { width: 100%; max-width: 300px; padding: .5rem; margin: .5rem 0 1rem; border: 1px solid #ccc; border-radius: 4px; }
  table { width: 100%; border-collapse: collapse; margin-bottom: 1rem; }
  th, td { border: 1px solid #F0F9FA; padding: .75rem; text-align: left; }
  th { background: #F0F9FA; }
  details.code-details { margin: 1rem 0; }
  summary { cursor: pointer; font-weight: 600; font-family: monospace; }
  .copy-btn { float: right; margin-top: -1.5rem; padding: .25rem .5rem; background: #4E2A84; color: #fff; border: none; border-radius: 4px; cursor: pointer; }
  pre { background: #F5F5F5; padding: 1rem; border-radius: 4px; overflow-x: auto; }
  .toggle-btn { margin: .5rem 0; padding: .5rem 1rem; background: #FFD100; border: none; border-radius: 4px; cursor: pointer; }
  .csv-sample { display: none; white-space: pre-wrap; }
</style>

<script>
  // Copy code snippets
  document.querySelectorAll('.copy-btn').forEach(btn => {
    btn.addEventListener('click', () => {
      const code = btn.nextElementSibling.innerText;
      navigator.clipboard.writeText(code);
      btn.textContent = 'Copied!';
      setTimeout(() => btn.textContent = 'Copy', 2000);
    });
  });

  // Table filter for Whisper variants
  document.getElementById('whisperFilter').addEventListener('input', e => {
    const filter = e.target.value.toLowerCase();
    document.querySelectorAll('#whisperTable tbody tr').forEach(row => {
      row.style.display = row.cells[0].innerText.toLowerCase().includes(filter)
        ? '' : 'none';
    });
  });

  // Toggle sample output
  document.querySelectorAll('.toggle-btn').forEach(btn => {
    btn.addEventListener('click', () => {
      const tbl = document.querySelector(btn.dataset.target);
      tbl.style.display = tbl.style.display === 'block' ? 'none' : 'block';
    });
  });
</script>
